该部分课程提供如下阅读文献，供同学们参考。 

[1] Anaby-Tavor A, Carmeli B, Goldbraich E, et al. Do not have enough data? Deep learning to the rescue![C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(05): 7383-7390.  
[2] De Angeli K, Gao S, Danciu I, et al. Class imbalance in out-of-distribution datasets: Improving the robustness of the TextCNN for the classification of rare cancer types[J]. Journal of biomedical informatics, 2022, 125: 103957.  
[3] Chen J, Yang Z, Yang D. Mixtext: Linguistically-informed interpolation of hidden space for semi-supervised text classification[J]. arXiv preprint arXiv:2004.12239, 2020.  
[4] Feng S Y, Gangal V, Kang D, et al. GenAug: Data Augmentation for Finetuning Text Generators[C]//Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. 2020: 29-42.  
[5] Guo H. Nonlinear mixup: Out-of-manifold data augmentation for text classification[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(04): 4044-4051.   
[6] Gururangan S, Dang T, Card D, et al. Variational Pretraining for Semi-supervised Text Classification[C]//Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019: 5880-5894.  
[7] Karimi A, Rossi L, Prati A. AEDA: An Easier Data Augmentation Technique for Text Classification[C]//Findings of the Association for Computational Linguistics: EMNLP 2021. 2021: 2748-2754.  
[8] Kedzie C, Mckeown K. A Good Sample is Hard to Find: Noise Injection Sampling and Self-Training for Neural Language Generation Models[C]//Proceedings of the 12th International Conference on Natural Language Generation. 2019: 584-593.  
[9] Kim D, Koo J, Kim U M. EnvBERT: Multi-Label Text Classification for Imbalanced, Noisy Environmental News Data[C]//2021 15th International Conference on Ubiquitous Information Management and Communication (IMCOM). IEEE, 2021: 1-8.  
[10] Kumar V, Choudhary A, Cho E. Data Augmentation using Pre-trained Transformer Models[C]//Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems. 2020: 18-26.  
[11] Li Y, Cohn T, Baldwin T. Robust training under linguistic adversity[C]//Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers. 2017: 21-27.  
[12] Liu R, Xu G, Jia C, et al. Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation[C]//Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020: 9031-9041.  
[13] Montella S, Fabre B, Urvoy T, et al. Denoising Pre-Training and Data Augmentation Strategies for Enhanced RDF Verbalization with Transformers[C]//Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+). 2020: 89-99.  
[14] Olsson V, Tranheden W, Pinto J, et al. Classmix: Segmentation-based data augmentation for semi-supervised learning[C]//Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2021: 1369-1378.  
[15] Padurariu C, Breaban M E. Dealing with data imbalance in text classification[J]. Procedia Computer Science, 2019, 159: 736-745.  
[16] Quteineh H, Samothrakis S, Sutcliffe R. Textual data augmentation for efficient active learning on tiny datasets[C]//Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2020: 7400-7410.  
[17] Ren S, Deng Y, He K, et al. Generating natural language adversarial examples through probability weighted word saliency[C]//Proceedings of the 57th annual meeting of the association for computational linguistics. 2019: 1085-1097.  
[18] Ren S, Zhang J, Li L, et al. Text AutoAugment: Learning Compositional Augmentation Policy for Text Classification[C]//Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021: 9029-9043.  
[19] Wei J, Zou K. Eda: Easy data augmentation techniques for boosting performance on text classification tasks[J]. arXiv preprint arXiv:1901.11196, 2019.  
[20] Zhang R, Yu Y, Zhang C. SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup[C]//Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020: 8566-8579.  
